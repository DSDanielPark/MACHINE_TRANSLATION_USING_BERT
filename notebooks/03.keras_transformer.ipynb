{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/nlpai-lab/nlp-bible-code/blob/master/25%EC%9E%A5_%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EB%B0%98%20%EA%B8%B0%EA%B3%84%EB%B2%88%EC%97%AD/%5B25-2%5DKeras%EB%A5%BC%20%EC%9D%B4%EC%9A%A9%ED%95%9C%20Transformer%20%EC%8B%A4%EC%8A%B5.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras-transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras_transformer import get_model\n",
    "\n",
    "#예시 문장\n",
    "tokens = '안녕하세요 저의 이름은 박찬준입니다. 만나서 반갑습니다. 저는 자연언어처리를 전공으로 하고 있습니다.'.split(' ')\n",
    "\n",
    "#토큰 딕셔너리 생성\n",
    "token_dict = {\n",
    "    '<PAD>': 0,\n",
    "    '<START>': 1,\n",
    "    '<END>': 2,\n",
    "}\n",
    "\n",
    "#예시문장 토큰화 및 딕셔너리화\n",
    "for token in tokens:\n",
    "    if token not in token_dict:\n",
    "        token_dict[token] = len(token_dict)\n",
    "\n",
    "#데이터 전처리 작업 (패딩 등)\n",
    "encoder_inputs_no_padding = []\n",
    "encoder_inputs, decoder_inputs, decoder_outputs = [], [], []\n",
    "\n",
    "for i in range(1, len(tokens) - 1):\n",
    "    encode_tokens, decode_tokens = tokens[:i], tokens[i:]\n",
    "    encode_tokens = ['<START>'] + encode_tokens + ['<END>'] + ['<PAD>'] * (len(tokens) - len(encode_tokens)) #패딩\n",
    "    \n",
    "    output_tokens = decode_tokens + ['<END>', '<PAD>'] + ['<PAD>'] * (len(tokens) - len(decode_tokens))\n",
    "    \n",
    "    decode_tokens = ['<START>'] + decode_tokens + ['<END>'] + ['<PAD>'] * (len(tokens) - len(decode_tokens))#패딩\n",
    "    \n",
    "    encode_tokens = list(map(lambda x: token_dict[x], encode_tokens))\n",
    "    decode_tokens = list(map(lambda x: token_dict[x], decode_tokens))\n",
    "    output_tokens = list(map(lambda x: [token_dict[x]], output_tokens))\n",
    "    \n",
    "    encoder_inputs_no_padding.append(encode_tokens[:i + 2])\n",
    "    encoder_inputs.append(encode_tokens)\n",
    "    \n",
    "    decoder_inputs.append(decode_tokens)\n",
    "    decoder_outputs.append(output_tokens)\n",
    "\n",
    "# 모델 생성 (keras_transformer 이용)\n",
    "model = get_model(\n",
    "    token_num=len(token_dict),\n",
    "    embed_dim=30,\n",
    "    encoder_num=3,\n",
    "    decoder_num=2,\n",
    "    head_num=3,\n",
    "    hidden_dim=120,\n",
    "    attention_activation='relu',\n",
    "    feed_forward_activation='relu',\n",
    "    dropout_rate=0.05,\n",
    "    embed_weights=np.random.random((14, 30)),\n",
    ")\n",
    "\n",
    "#모델 컴파일\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    ")\n",
    "\n",
    "#모델 써머리\n",
    "model.summary()\n",
    "\n",
    "# 모델  훈련\n",
    "model.fit(\n",
    "    x=[np.asarray(encoder_inputs * 1000), np.asarray(decoder_inputs * 1000)],\n",
    "    y=np.asarray(decoder_outputs * 1000),\n",
    "    epochs=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras_transformer import get_model, decode\n",
    "\n",
    "#소스 문장\n",
    "source_tokens = [\n",
    "    '안녕하세요 저의 이름은 박찬준입니다.'.split(' '),\n",
    "    '저는 24살입니다.'.split(' '),\n",
    "]\n",
    "\n",
    "#타겟 문장\n",
    "target_tokens = [\n",
    "    list('Hello My name is Park Chanjun.'),\n",
    "    list('I am 24 years old.'),\n",
    "]\n",
    "\n",
    "#토큰 딕셔너리화 함수\n",
    "def build_token_dict(token_list):\n",
    "    token_dict = {\n",
    "        '<PAD>': 0,\n",
    "        '<START>': 1,\n",
    "        '<END>': 2,\n",
    "    }\n",
    "    for tokens in token_list:\n",
    "        for token in tokens:\n",
    "            if token not in token_dict:\n",
    "                token_dict[token] = len(token_dict)\n",
    "    return token_dict\n",
    "\n",
    "\n",
    "source_token_dict = build_token_dict(source_tokens) #딕셔너리화\n",
    "target_token_dict = build_token_dict(target_tokens) #딕셔너리화\n",
    "target_token_dict_inv = {v: k for k, v in target_token_dict.items()} #역으로.\n",
    "\n",
    "# <START>,<END>와 같은 Special Token 추가\n",
    "encode_tokens = [['<START>'] + tokens + ['<END>'] for tokens in source_tokens]\n",
    "decode_tokens = [['<START>'] + tokens + ['<END>'] for tokens in target_tokens]\n",
    "output_tokens = [tokens + ['<END>', '<PAD>'] for tokens in target_tokens]\n",
    "\n",
    "# 패딩\n",
    "source_max_len = max(map(len, encode_tokens))\n",
    "target_max_len = max(map(len, decode_tokens))\n",
    "\n",
    "encode_tokens = [tokens + ['<PAD>'] * (source_max_len - len(tokens)) for tokens in encode_tokens]\n",
    "decode_tokens = [tokens + ['<PAD>'] * (target_max_len - len(tokens)) for tokens in decode_tokens]\n",
    "output_tokens = [tokens + ['<PAD>'] * (target_max_len - len(tokens)) for tokens in output_tokens]\n",
    "\n",
    "encode_input = [list(map(lambda x: source_token_dict[x], tokens)) for tokens in encode_tokens]\n",
    "decode_input = [list(map(lambda x: target_token_dict[x], tokens)) for tokens in decode_tokens]\n",
    "decode_output = [list(map(lambda x: [target_token_dict[x]], tokens)) for tokens in output_tokens]\n",
    "\n",
    "\n",
    "#모델 생성\n",
    "model = get_model(\n",
    "    token_num=max(len(source_token_dict), len(target_token_dict)),\n",
    "    embed_dim=32,\n",
    "    encoder_num=2,\n",
    "    decoder_num=2,\n",
    "    head_num=4,\n",
    "    hidden_dim=128,\n",
    "    dropout_rate=0.05,\n",
    "    use_same_embed=False,  # Use different embeddings for different languages\n",
    ")\n",
    "\n",
    "#모델 컴파일\n",
    "model.compile('adam', 'sparse_categorical_crossentropy')\n",
    "\n",
    "#모델 써머리 \n",
    "model.summary()\n",
    "\n",
    "#모델 훈련\n",
    "model.fit(\n",
    "    x=[np.array(encode_input * 1024), np.array(decode_input * 1024)],\n",
    "    y=np.array(decode_output * 1024),\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    ")\n",
    "\n",
    "# 번역 진행 (Predict)\n",
    "decoded = decode(\n",
    "    model,\n",
    "    encode_input,\n",
    "    start_token=target_token_dict['<START>'],\n",
    "    end_token=target_token_dict['<END>'],\n",
    "    pad_token=target_token_dict['<PAD>'],\n",
    ")\n",
    "\n",
    "print(''.join(map(lambda x: target_token_dict_inv[x], decoded[0][1:-1])))\n",
    "print(''.join(map(lambda x: target_token_dict_inv[x], decoded[1][1:-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
